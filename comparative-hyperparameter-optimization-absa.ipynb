{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Comparative Hyperparameter Optimization for Aspect-Based Sentiment Analysis\n\nThis notebook implements a comprehensive comparison of ABSA models:\n- **Models:** Random Forest vs XGBoost\n- **Feature Extraction:** c-TF-IDF vs TF-IDF with N-grams (bigrams, trigrams)\n- **Sampling Methods:** SMOTE vs ADASYN\n- **Hyperparameter Optimization:** RandomSearchCV, Bayesian Optimization, Optuna","metadata":{"editable":false}},{"cell_type":"code","source":"!pip install pandas numpy scikit-learn optuna xgboost imblearn ","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1. Import Required Libraries","metadata":{"editable":false}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import (\n    confusion_matrix, classification_report, roc_auc_score, \n    roc_curve, auc, accuracy_score, precision_recall_fscore_support\n)\nfrom imblearn.over_sampling import SMOTE, ADASYN\nfrom scipy.stats import randint, uniform\nimport optuna\nfrom optuna.samplers import TPESampler\nimport warnings\nimport os\nfrom datetime import datetime\nimport json\nimport pickle\n\nwarnings.filterwarnings('ignore')\n\n# Set random seed for reproducibility\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Class-based TF-IDF (c-TF-IDF) Implementation","metadata":{"editable":false}},{"cell_type":"code","source":"class CTFIDFVectorizer:\n    \"\"\"Class-based TF-IDF (c-TF-IDF) implementation\"\"\"\n    \n    def __init__(self, ngram_range=(1, 1), max_features=None):\n        self.ngram_range = ngram_range\n        self.max_features = max_features\n        self.vectorizer = TfidfVectorizer(ngram_range=ngram_range, max_features=max_features)\n        self.class_docs = {}\n        \n    def fit(self, X, y):\n        \"\"\"Fit c-TF-IDF by creating class-level documents\"\"\"\n        # Create class-level documents by concatenating all documents in each class\n        for label in np.unique(y):\n            class_texts = X[y == label]\n            # Filter out NaN and convert to string\n            valid_texts = [str(text) for text in class_texts if pd.notna(text) and str(text).strip()]\n            self.class_docs[label] = ' '.join(valid_texts) if valid_texts else ''\n        \n        # Fit vectorizer on class documents\n        class_texts_list = [self.class_docs[label] for label in sorted(self.class_docs.keys())]\n        self.vectorizer.fit(class_texts_list)\n        return self\n    \n    def transform(self, X):\n        \"\"\"Transform documents using c-TF-IDF\"\"\"\n        return self.vectorizer.transform(X)\n    \n    def fit_transform(self, X, y):\n        \"\"\"Fit and transform\"\"\"\n        self.fit(X, y)\n        return self.transform(X)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Comparative ABSA Main Class","metadata":{"editable":false}},{"cell_type":"code","source":"class ComparativeABSA:\n    \"\"\"Comparative Aspect-Based Sentiment Analysis with Hyperparameter Optimization\"\"\"\n    \n    def __init__(self, output_dir='results'):\n        self.output_dir = output_dir\n        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n        self.results = {}\n        self.models = {}\n        self.vectorizers = {}\n        \n        # Create output directories\n        os.makedirs(output_dir, exist_ok=True)\n        os.makedirs(f'{output_dir}/visualizations', exist_ok=True)\n        os.makedirs(f'{output_dir}/models', exist_ok=True)\n    \n    @staticmethod\n    def df_to_markdown(df, float_format='.4f'):\n        \"\"\"Convert DataFrame to markdown table format\"\"\"\n        # Get column names\n        cols = df.columns.tolist()\n        \n        # Create header\n        header = '| ' + ' | '.join(str(col) for col in cols) + ' |'\n        separator = '| ' + ' | '.join(['---' for _ in cols]) + ' |'\n        \n        # Create rows\n        rows = []\n        for _, row in df.iterrows():\n            formatted_values = []\n            for val in row:\n                if isinstance(val, float):\n                    formatted_values.append(f'{val:{float_format}}')\n                else:\n                    formatted_values.append(str(val))\n            rows.append('| ' + ' | '.join(formatted_values) + ' |')\n        \n        return '\\n'.join([header, separator] + rows)\n        \n    def load_and_prepare_data(self, filepath, sample_size_per_bank=1000):\n        \"\"\"Load dataset and create balanced sample\"\"\"\n        print(f\"Loading dataset from {filepath}...\")\n        df = pd.read_csv(filepath, compression='gzip', encoding='utf-8', on_bad_lines='skip')\n        \n        print(f\"Original dataset shape: {df.shape}\")\n        print(f\"\\nBank distribution:\\n{df['bank_name'].value_counts()}\")\n        print(f\"\\nSentiment distribution:\\n{df['sentiment'].value_counts()}\")\n        \n        # Create balanced sample: balanced by bank and sentiment\n        balanced_samples = []\n        \n        for bank in df['bank_name'].unique():\n            bank_data = df[df['bank_name'] == bank]\n            \n            # Calculate samples per sentiment for this bank\n            sentiments = bank_data['sentiment'].unique()\n            samples_per_sentiment = sample_size_per_bank // len(sentiments)\n            \n            for sentiment in sentiments:\n                sentiment_data = bank_data[bank_data['sentiment'] == sentiment]\n                \n                # Sample with replacement if not enough data\n                if len(sentiment_data) >= samples_per_sentiment:\n                    sampled = sentiment_data.sample(n=samples_per_sentiment, random_state=RANDOM_STATE)\n                else:\n                    sampled = sentiment_data.sample(n=samples_per_sentiment, replace=True, random_state=RANDOM_STATE)\n                \n                balanced_samples.append(sampled)\n        \n        # Combine all samples\n        balanced_df = pd.concat(balanced_samples, ignore_index=True)\n        \n        # Remove duplicates\n        balanced_df = balanced_df.drop_duplicates(subset=['content_stemmed'])\n        \n        # Remove rows with NaN or empty content_stemmed\n        balanced_df = balanced_df[balanced_df['content_stemmed'].notna()]\n        balanced_df = balanced_df[balanced_df['content_stemmed'].str.strip() != '']\n        \n        print(f\"\\nBalanced dataset shape: {balanced_df.shape}\")\n        print(f\"\\nBalanced bank distribution:\\n{balanced_df['bank_name'].value_counts()}\")\n        print(f\"\\nBalanced sentiment distribution:\\n{balanced_df['sentiment'].value_counts()}\")\n        \n        return balanced_df\n    \n    def create_features(self, X_train, X_test, y_train, feature_type='tfidf', ngram_range=(1, 2)):\n        \"\"\"Create features using TF-IDF or c-TF-IDF\"\"\"\n        print(f\"\\nCreating features with {feature_type} and n-gram range {ngram_range}...\")\n        \n        if feature_type == 'ctfidf':\n            vectorizer = CTFIDFVectorizer(ngram_range=ngram_range, max_features=5000)\n        else:  # tfidf\n            vectorizer = TfidfVectorizer(ngram_range=ngram_range, max_features=5000)\n        \n        if feature_type == 'ctfidf':\n            X_train_vec = vectorizer.fit_transform(X_train.values, y_train.values)\n        else:\n            X_train_vec = vectorizer.fit_transform(X_train.values)\n        \n        X_test_vec = vectorizer.transform(X_test.values)\n        \n        print(f\"Feature matrix shape: {X_train_vec.shape}\")\n        \n        return X_train_vec, X_test_vec, vectorizer\n    \n    def apply_sampling(self, X_train, y_train, sampling_method='smote'):\n        \"\"\"Apply SMOTE or ADASYN sampling\"\"\"\n        print(f\"\\nApplying {sampling_method.upper()} sampling...\")\n        print(f\"Before sampling: {y_train.value_counts().to_dict()}\")\n        \n        if sampling_method == 'smote':\n            sampler = SMOTE(random_state=RANDOM_STATE, k_neighbors=5)\n        else:  # adasyn\n            sampler = ADASYN(random_state=RANDOM_STATE, n_neighbors=5)\n        \n        X_resampled, y_resampled = sampler.fit_resample(X_train, y_train)\n        \n        print(f\"After sampling: {pd.Series(y_resampled).value_counts().to_dict()}\")\n        \n        return X_resampled, y_resampled\n    \n    def optimize_random_forest_random_search(self, X_train, y_train):\n        \"\"\"Optimize Random Forest using RandomizedSearchCV\"\"\"\n        print(\"\\nOptimizing Random Forest with RandomizedSearchCV...\")\n        \n        param_distributions = {\n            'n_estimators': randint(100, 500),\n            'max_depth': [10, 20, 30, 40, 50, None],\n            'min_samples_split': randint(2, 20),\n            'min_samples_leaf': randint(1, 10),\n            'max_features': ['sqrt', 'log2', None],\n            'bootstrap': [True, False],\n            'class_weight': ['balanced', 'balanced_subsample', None]\n        }\n        \n        rf = RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1)\n        \n        random_search = RandomizedSearchCV(\n            rf, param_distributions, n_iter=50, cv=3, \n            scoring='f1_weighted', random_state=RANDOM_STATE, \n            n_jobs=-1, verbose=1\n        )\n        \n        random_search.fit(X_train, y_train)\n        \n        print(f\"Best parameters: {random_search.best_params_}\")\n        print(f\"Best CV score: {random_search.best_score_:.4f}\")\n        \n        return random_search.best_estimator_, random_search.best_params_\n    \n    def optimize_xgboost_optuna(self, X_train, y_train):\n        \"\"\"Optimize XGBoost using Optuna\"\"\"\n        print(\"\\nOptimizing XGBoost with Optuna...\")\n        \n        # Encode labels for XGBoost\n        from sklearn.preprocessing import LabelEncoder\n        le = LabelEncoder()\n        y_train_encoded = le.fit_transform(y_train)\n        \n        def objective(trial):\n            params = {\n                'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n                'max_depth': trial.suggest_int('max_depth', 3, 15),\n                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n                'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n                'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n                'gamma': trial.suggest_float('gamma', 0, 5),\n                'reg_alpha': trial.suggest_float('reg_alpha', 0, 2),\n                'reg_lambda': trial.suggest_float('reg_lambda', 0, 2),\n                'random_state': RANDOM_STATE,\n                'n_jobs': -1,\n                'tree_method': 'hist'\n            }\n            \n            model = XGBClassifier(**params)\n            \n            # Cross-validation\n            from sklearn.model_selection import cross_val_score\n            scores = cross_val_score(model, X_train, y_train_encoded, cv=3, \n                                    scoring='f1_weighted', n_jobs=-1)\n            \n            return scores.mean()\n        \n        study = optuna.create_study(direction='maximize', sampler=TPESampler(seed=RANDOM_STATE))\n        study.optimize(objective, n_trials=50, show_progress_bar=True)\n        \n        print(f\"Best parameters: {study.best_params}\")\n        print(f\"Best CV score: {study.best_value:.4f}\")\n        \n        # Train final model with best parameters\n        best_model = XGBClassifier(**study.best_params)\n        best_model.fit(X_train, y_train_encoded)\n        \n        # Store label encoder for later use\n        best_model.label_encoder = le\n        \n        return best_model, study.best_params\n    \n    def evaluate_model(self, model, X_test, y_test, model_name):\n        \"\"\"Evaluate model and generate metrics\"\"\"\n        print(f\"\\nEvaluating {model_name}...\")\n        \n        # Handle XGBoost predictions\n        if hasattr(model, 'label_encoder'):\n            y_test_encoded = model.label_encoder.transform(y_test)\n            y_pred_encoded = model.predict(X_test)\n            y_pred = model.label_encoder.inverse_transform(y_pred_encoded)\n            y_pred_proba = model.predict_proba(X_test)\n        else:\n            y_pred = model.predict(X_test)\n            y_pred_proba = model.predict_proba(X_test)\n        \n        # Calculate metrics\n        accuracy = accuracy_score(y_test, y_pred)\n        precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n        \n        # Classification report\n        report = classification_report(y_test, y_pred, output_dict=True)\n        \n        # Confusion matrix\n        cm = confusion_matrix(y_test, y_pred)\n        \n        # ROC AUC (multiclass)\n        from sklearn.preprocessing import label_binarize\n        classes = np.unique(y_test)\n        y_test_bin = label_binarize(y_test, classes=classes)\n        \n        # Calculate ROC AUC for each class\n        roc_auc_dict = {}\n        fpr_dict = {}\n        tpr_dict = {}\n        \n        for i, class_name in enumerate(classes):\n            fpr_dict[class_name], tpr_dict[class_name], _ = roc_curve(y_test_bin[:, i], y_pred_proba[:, i])\n            roc_auc_dict[class_name] = auc(fpr_dict[class_name], tpr_dict[class_name])\n        \n        # Macro average ROC AUC\n        roc_auc_macro = roc_auc_score(y_test_bin, y_pred_proba, average='macro', multi_class='ovr')\n        \n        results = {\n            'model_name': model_name,\n            'accuracy': accuracy,\n            'precision': precision,\n            'recall': recall,\n            'f1_score': f1,\n            'roc_auc_macro': roc_auc_macro,\n            'roc_auc_per_class': roc_auc_dict,\n            'confusion_matrix': cm,\n            'classification_report': report,\n            'fpr': fpr_dict,\n            'tpr': tpr_dict,\n            'classes': classes\n        }\n        \n        print(f\"Accuracy: {accuracy:.4f}\")\n        print(f\"Precision: {precision:.4f}\")\n        print(f\"Recall: {recall:.4f}\")\n        print(f\"F1-Score: {f1:.4f}\")\n        print(f\"ROC AUC (Macro): {roc_auc_macro:.4f}\")\n        \n        return results\n    \n    def plot_confusion_matrix(self, cm, classes, model_name, filename):\n        \"\"\"Plot confusion matrix\"\"\"\n        plt.figure(figsize=(10, 8))\n        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n                   xticklabels=classes, yticklabels=classes, cbar_kws={'label': 'Count'})\n        plt.title(f'Confusion Matrix - {model_name}', fontsize=14, fontweight='bold')\n        plt.ylabel('True Label', fontsize=12)\n        plt.xlabel('Predicted Label', fontsize=12)\n        plt.tight_layout()\n        plt.savefig(filename, dpi=300, bbox_inches='tight')\n        plt.close()\n        print(f\"Saved confusion matrix: {filename}\")\n    \n    def plot_roc_curve(self, results_list, filename):\n        \"\"\"Plot ROC curves for multiple models\"\"\"\n        plt.figure(figsize=(12, 8))\n        \n        colors = plt.cm.Set3(np.linspace(0, 1, len(results_list)))\n        \n        for idx, result in enumerate(results_list):\n            model_name = result['model_name']\n            classes = result['classes']\n            \n            # Plot ROC curve for each class\n            for i, class_name in enumerate(classes):\n                fpr = result['fpr'][class_name]\n                tpr = result['tpr'][class_name]\n                roc_auc = result['roc_auc_per_class'][class_name]\n                \n                label = f\"{model_name} - {class_name} (AUC = {roc_auc:.3f})\"\n                plt.plot(fpr, tpr, color=colors[idx], alpha=0.7, \n                        linestyle=['-', '--', '-.'][i], linewidth=2, label=label)\n        \n        plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier')\n        plt.xlim([0.0, 1.0])\n        plt.ylim([0.0, 1.05])\n        plt.xlabel('False Positive Rate', fontsize=12)\n        plt.ylabel('True Positive Rate', fontsize=12)\n        plt.title('ROC Curves - Model Comparison', fontsize=14, fontweight='bold')\n        plt.legend(loc='lower right', fontsize=9)\n        plt.grid(alpha=0.3)\n        plt.tight_layout()\n        plt.savefig(filename, dpi=300, bbox_inches='tight')\n        plt.close()\n        print(f\"Saved ROC curve: {filename}\")\n    \n    def plot_metrics_comparison(self, results_df, filename):\n        \"\"\"Plot comparison of metrics across models\"\"\"\n        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n        \n        metrics = ['accuracy', 'precision', 'recall', 'f1_score']\n        titles = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n        \n        for idx, (metric, title) in enumerate(zip(metrics, titles)):\n            ax = axes[idx // 2, idx % 2]\n            \n            data = results_df.pivot_table(\n                index=['feature_type', 'ngram'], \n                columns='sampling_method', \n                values=metric\n            )\n            \n            data.plot(kind='bar', ax=ax, width=0.8, colormap='Set2')\n            ax.set_title(f'{title} Comparison', fontsize=12, fontweight='bold')\n            ax.set_ylabel(title, fontsize=11)\n            ax.set_xlabel('Feature Type & N-gram', fontsize=11)\n            ax.legend(title='Sampling Method', fontsize=9)\n            ax.grid(axis='y', alpha=0.3)\n            ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n        \n        plt.tight_layout()\n        plt.savefig(filename, dpi=300, bbox_inches='tight')\n        plt.close()\n        print(f\"Saved metrics comparison: {filename}\")\n    \n    def run_experiment(self, df, test_size=0.3, feature_configs=None, sampling_methods=None):\n        \"\"\"Run complete experiment with all configurations\"\"\"\n        \n        if feature_configs is None:\n            feature_configs = [\n                ('tfidf', (2, 2), 'bigram'),\n                ('tfidf', (3, 3), 'trigram'),\n                ('ctfidf', (2, 2), 'bigram'),\n                ('ctfidf', (3, 3), 'trigram')\n            ]\n        \n        if sampling_methods is None:\n            sampling_methods = ['smote', 'adasyn']\n        \n        # Prepare data\n        X = df['content_stemmed']\n        y = df['sentiment']\n        \n        # Split data\n        X_train, X_test, y_train, y_test = train_test_split(\n            X, y, test_size=test_size, random_state=RANDOM_STATE, stratify=y\n        )\n        \n        print(f\"\\n{'='*80}\")\n        print(f\"Train/Test Split: {int((1-test_size)*100)}/{int(test_size*100)}\")\n        print(f\"Training set size: {len(X_train)}\")\n        print(f\"Test set size: {len(X_test)}\")\n        print(f\"{'='*80}\")\n        \n        all_results = []\n        experiment_id = 0\n        \n        # Iterate through all configurations\n        for feature_type, ngram_range, ngram_name in feature_configs:\n            # Create features\n            X_train_vec, X_test_vec, vectorizer = self.create_features(\n                X_train, X_test, y_train, feature_type, ngram_range\n            )\n            \n            for sampling_method in sampling_methods:\n                # Apply sampling\n                X_train_resampled, y_train_resampled = self.apply_sampling(\n                    X_train_vec, y_train, sampling_method\n                )\n                \n                print(f\"\\n{'='*80}\")\n                print(f\"Configuration: {feature_type.upper()} + {ngram_name.upper()} + {sampling_method.upper()}\")\n                print(f\"{'='*80}\")\n                \n                # Train and evaluate Random Forest\n                rf_model, rf_params = self.optimize_random_forest_random_search(\n                    X_train_resampled, y_train_resampled\n                )\n                \n                rf_results = self.evaluate_model(\n                    rf_model, X_test_vec, y_test,\n                    f\"RF_{feature_type}_{ngram_name}_{sampling_method}\"\n                )\n                \n                rf_results.update({\n                    'model_type': 'RandomForest',\n                    'feature_type': feature_type,\n                    'ngram': ngram_name,\n                    'sampling_method': sampling_method,\n                    'test_size': test_size,\n                    'hyperparameters': rf_params,\n                    'optimization_method': 'RandomSearchCV'\n                })\n                \n                all_results.append(rf_results)\n                \n                # Save confusion matrix\n                self.plot_confusion_matrix(\n                    rf_results['confusion_matrix'],\n                    rf_results['classes'],\n                    rf_results['model_name'],\n                    f\"{self.output_dir}/visualizations/cm_{rf_results['model_name']}.png\"\n                )\n                \n                # Train and evaluate XGBoost\n                xgb_model, xgb_params = self.optimize_xgboost_optuna(\n                    X_train_resampled, y_train_resampled\n                )\n                \n                xgb_results = self.evaluate_model(\n                    xgb_model, X_test_vec, y_test,\n                    f\"XGB_{feature_type}_{ngram_name}_{sampling_method}\"\n                )\n                \n                xgb_results.update({\n                    'model_type': 'XGBoost',\n                    'feature_type': feature_type,\n                    'ngram': ngram_name,\n                    'sampling_method': sampling_method,\n                    'test_size': test_size,\n                    'hyperparameters': xgb_params,\n                    'optimization_method': 'Optuna'\n                })\n                \n                all_results.append(xgb_results)\n                \n                # Save confusion matrix\n                self.plot_confusion_matrix(\n                    xgb_results['confusion_matrix'],\n                    xgb_results['classes'],\n                    xgb_results['model_name'],\n                    f\"{self.output_dir}/visualizations/cm_{xgb_results['model_name']}.png\"\n                )\n                \n                # Save models\n                model_filename_rf = f\"{self.output_dir}/models/{rf_results['model_name']}.pkl\"\n                model_filename_xgb = f\"{self.output_dir}/models/{xgb_results['model_name']}.pkl\"\n                \n                with open(model_filename_rf, 'wb') as f:\n                    pickle.dump(rf_model, f)\n                with open(model_filename_xgb, 'wb') as f:\n                    pickle.dump(xgb_model, f)\n                \n                experiment_id += 2\n        \n        return all_results\n    \n    def generate_report(self, all_results, split_ratio):\n        \"\"\"Generate comprehensive markdown report\"\"\"\n        \n        # Convert results to DataFrame\n        results_data = []\n        for result in all_results:\n            results_data.append({\n                'Model': result['model_type'],\n                'Feature Type': result['feature_type'],\n                'N-gram': result['ngram'],\n                'Sampling': result['sampling_method'],\n                'Optimization': result['optimization_method'],\n                'Accuracy': result['accuracy'],\n                'Precision': result['precision'],\n                'Recall': result['recall'],\n                'F1-Score': result['f1_score'],\n                'ROC AUC': result['roc_auc_macro'],\n                'Test Size': result['test_size']\n            })\n        \n        results_df = pd.DataFrame(results_data)\n        \n        # Save to CSV\n        csv_filename = f\"{self.output_dir}/results_comparison_{split_ratio}.csv\"\n        results_df.to_csv(csv_filename, index=False)\n        print(f\"\\nSaved results to: {csv_filename}\")\n        \n        # Generate visualizations\n        self.plot_metrics_comparison(\n            results_df,\n            f\"{self.output_dir}/visualizations/metrics_comparison_{split_ratio}.png\"\n        )\n        \n        self.plot_roc_curve(\n            all_results,\n            f\"{self.output_dir}/visualizations/roc_curves_{split_ratio}.png\"\n        )\n        \n        # Generate markdown report\n        md_filename = f\"{self.output_dir}/report_{split_ratio}.md\"\n        \n        with open(md_filename, 'w', encoding='utf-8') as f:\n            f.write(f\"# Comparative Hyperparameter Optimization Report\\n\\n\")\n            f.write(f\"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n            f.write(f\"**Train/Test Split:** {split_ratio}\\n\\n\")\n            \n            f.write(\"## Executive Summary\\n\\n\")\n            f.write(\"This report presents a comprehensive comparison of Aspect-Based Sentiment Analysis (ABSA) models using:\\n\\n\")\n            f.write(\"- **Models:** Random Forest vs XGBoost\\n\")\n            f.write(\"- **Feature Extraction:** TF-IDF vs c-TF-IDF with Bigrams and Trigrams\\n\")\n            f.write(\"- **Sampling Methods:** SMOTE vs ADASYN\\n\")\n            f.write(\"- **Hyperparameter Optimization:** RandomSearchCV (RF) and Optuna (XGBoost)\\n\\n\")\n            \n            f.write(\"## Dataset Information\\n\\n\")\n            f.write(f\"- **Total Samples:** {len(results_df) // 8} configurations tested\\n\")\n            f.write(f\"- **Test Size:** {results_df['Test Size'].iloc[0] * 100:.0f}%\\n\")\n            f.write(f\"- **Random State:** {RANDOM_STATE}\\n\\n\")\n            \n            f.write(\"## Overall Results\\n\\n\")\n            \n            # Best model overall\n            best_idx = results_df['F1-Score'].idxmax()\n            best_model = results_df.iloc[best_idx]\n            \n            f.write(f\"### Best Performing Model\\n\\n\")\n            f.write(f\"- **Model:** {best_model['Model']}\\n\")\n            f.write(f\"- **Feature Type:** {best_model['Feature Type']}\\n\")\n            f.write(f\"- **N-gram:** {best_model['N-gram']}\\n\")\n            f.write(f\"- **Sampling:** {best_model['Sampling']}\\n\")\n            f.write(f\"- **F1-Score:** {best_model['F1-Score']:.4f}\\n\")\n            f.write(f\"- **Accuracy:** {best_model['Accuracy']:.4f}\\n\")\n            f.write(f\"- **ROC AUC:** {best_model['ROC AUC']:.4f}\\n\\n\")\n            \n            f.write(\"## Detailed Results Table\\n\\n\")\n            f.write(self.df_to_markdown(results_df, float_format='.4f'))\n            f.write(\"\\n\\n\")\n            \n            f.write(\"## Model Comparison by Configuration\\n\\n\")\n            \n            # Group by feature type and sampling\n            for feature_type in results_df['Feature Type'].unique():\n                f.write(f\"### {feature_type.upper()} Features\\n\\n\")\n                \n                for sampling in results_df['Sampling'].unique():\n                    subset = results_df[\n                        (results_df['Feature Type'] == feature_type) & \n                        (results_df['Sampling'] == sampling)\n                    ]\n                    \n                    f.write(f\"#### {sampling.upper()} Sampling\\n\\n\")\n                    f.write(self.df_to_markdown(subset[['Model', 'N-gram', 'Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC AUC']], float_format='.4f'))\n                    f.write(\"\\n\\n\")\n            \n            f.write(\"## Visualizations\\n\\n\")\n            f.write(f\"![Metrics Comparison](visualizations/metrics_comparison_{split_ratio}.png)\\n\\n\")\n            f.write(f\"![ROC Curves](visualizations/roc_curves_{split_ratio}.png)\\n\\n\")\n            \n            f.write(\"## Confusion Matrices\\n\\n\")\n            for result in all_results:\n                model_name = result['model_name']\n                f.write(f\"### {model_name}\\n\\n\")\n                f.write(f\"![Confusion Matrix](visualizations/cm_{model_name}.png)\\n\\n\")\n            \n            f.write(\"## Detailed Classification Reports\\n\\n\")\n            for result in all_results:\n                f.write(f\"### {result['model_name']}\\n\\n\")\n                report_df = pd.DataFrame(result['classification_report']).transpose()\n                f.write(self.df_to_markdown(report_df, float_format='.4f'))\n                f.write(\"\\n\\n\")\n            \n            f.write(\"## Hyperparameter Configurations\\n\\n\")\n            for result in all_results:\n                f.write(f\"### {result['model_name']}\\n\\n\")\n                f.write(f\"**Optimization Method:** {result['optimization_method']}\\n\\n\")\n                f.write(\"**Best Hyperparameters:**\\n\\n\")\n                for param, value in result['hyperparameters'].items():\n                    f.write(f\"- `{param}`: {value}\\n\")\n                f.write(\"\\n\")\n            \n            f.write(\"## Conclusions\\n\\n\")\n            f.write(\"### Key Findings\\n\\n\")\n            \n            # Compare RF vs XGBoost\n            rf_avg = results_df[results_df['Model'] == 'RandomForest']['F1-Score'].mean()\n            xgb_avg = results_df[results_df['Model'] == 'XGBoost']['F1-Score'].mean()\n            \n            f.write(f\"1. **Model Performance:**\\n\")\n            f.write(f\"   - Random Forest Average F1-Score: {rf_avg:.4f}\\n\")\n            f.write(f\"   - XGBoost Average F1-Score: {xgb_avg:.4f}\\n\")\n            f.write(f\"   - Winner: {'Random Forest' if rf_avg > xgb_avg else 'XGBoost'}\\n\\n\")\n            \n            # Compare feature types\n            tfidf_avg = results_df[results_df['Feature Type'] == 'tfidf']['F1-Score'].mean()\n            ctfidf_avg = results_df[results_df['Feature Type'] == 'ctfidf']['F1-Score'].mean()\n            \n            f.write(f\"2. **Feature Extraction:**\\n\")\n            f.write(f\"   - TF-IDF Average F1-Score: {tfidf_avg:.4f}\\n\")\n            f.write(f\"   - c-TF-IDF Average F1-Score: {ctfidf_avg:.4f}\\n\")\n            f.write(f\"   - Winner: {'TF-IDF' if tfidf_avg > ctfidf_avg else 'c-TF-IDF'}\\n\\n\")\n            \n            # Compare sampling methods\n            smote_avg = results_df[results_df['Sampling'] == 'smote']['F1-Score'].mean()\n            adasyn_avg = results_df[results_df['Sampling'] == 'adasyn']['F1-Score'].mean()\n            \n            f.write(f\"3. **Sampling Methods:**\\n\")\n            f.write(f\"   - SMOTE Average F1-Score: {smote_avg:.4f}\\n\")\n            f.write(f\"   - ADASYN Average F1-Score: {adasyn_avg:.4f}\\n\")\n            f.write(f\"   - Winner: {'SMOTE' if smote_avg > adasyn_avg else 'ADASYN'}\\n\\n\")\n            \n            # Compare n-grams\n            bigram_avg = results_df[results_df['N-gram'] == 'bigram']['F1-Score'].mean()\n            trigram_avg = results_df[results_df['N-gram'] == 'trigram']['F1-Score'].mean()\n            \n            f.write(f\"4. **N-gram Analysis:**\\n\")\n            f.write(f\"   - Bigram Average F1-Score: {bigram_avg:.4f}\\n\")\n            f.write(f\"   - Trigram Average F1-Score: {trigram_avg:.4f}\\n\")\n            f.write(f\"   - Winner: {'Bigram' if bigram_avg > trigram_avg else 'Trigram'}\\n\\n\")\n            \n            f.write(\"### Recommendations\\n\\n\")\n            f.write(f\"Based on the experimental results, the recommended configuration is:\\n\\n\")\n            f.write(f\"- **Model:** {best_model['Model']}\\n\")\n            f.write(f\"- **Feature Extraction:** {best_model['Feature Type'].upper()} with {best_model['N-gram']}\\n\")\n            f.write(f\"- **Sampling Method:** {best_model['Sampling'].upper()}\\n\")\n            f.write(f\"- **Expected F1-Score:** {best_model['F1-Score']:.4f}\\n\\n\")\n            \n            f.write(\"---\\n\\n\")\n            f.write(f\"*Report generated by Comparative ABSA System on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\\n\")\n        \n        print(f\"\\nSaved report to: {md_filename}\")\n        \n        return results_df, md_filename","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Configuration and Setup","metadata":{"editable":false}},{"cell_type":"code","source":"# Configuration\n# DATASET_PATH = 'dataset/07_indobert_filtered_90K_Stemmed.csv.gz'\nDATASET_PATH='https://github.com/roniwahyu/Indonesian-Digital-Banks-Dataset2025/raw/refs/heads/main/07_indobert_filtered_90K_Stemmed.csv.gz'\nSAMPLE_SIZE_PER_BANK = 1000  # Balanced samples per bank\nTEST_SIZES = [0.30, 0.25, 0.35]  # Multiple train/test splits\n\nprint(\"=\"*80)\nprint(\"COMPARATIVE HYPERPARAMETER OPTIMIZATION FOR ABSA\")\nprint(\"=\"*80)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Initialize System and Load Data","metadata":{"editable":false}},{"cell_type":"code","source":"# Initialize system\nabsa = ComparativeABSA(output_dir='results_absa_comparison')\n\n# Load and prepare data\ndf = absa.load_and_prepare_data(DATASET_PATH, sample_size_per_bank=SAMPLE_SIZE_PER_BANK)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#df statistics\ndf.info()","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#statistics by sentiment and banks and count total per sentiment\ndf.head()\n\ndf.groupby(['sentiment', 'bank_name']).size().unstack().fillna(0).astype(int)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.sentiment.value_counts()","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. Run Experiments for Each Train/Test Split","metadata":{"editable":false}},{"cell_type":"code","source":"# Run experiments for each train/test split\nall_split_results = {}\n\nfor test_size in TEST_SIZES:\n    split_ratio = f\"{int((1-test_size)*100)}_{int(test_size*100)}\"\n    print(f\"\\n{'='*80}\")\n    print(f\"RUNNING EXPERIMENTS WITH {split_ratio} SPLIT\")\n    print(f\"{'='*80}\")\n    \n    # Run experiment\n    results = absa.run_experiment(df, test_size=test_size)\n    \n    # Generate report\n    results_df, report_file = absa.generate_report(results, split_ratio)\n    \n    all_split_results[split_ratio] = {\n        'results': results,\n        'dataframe': results_df,\n        'report_file': report_file\n    }","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. Generate Summary Comparison Across All Splits","metadata":{"editable":false}},{"cell_type":"code","source":"print(f\"\\n{'='*80}\")\nprint(\"GENERATING SUMMARY COMPARISON\")\nprint(f\"{'='*80}\")\n\nsummary_data = []\nfor split_ratio, data in all_split_results.items():\n    df_results = data['dataframe']\n    best_idx = df_results['F1-Score'].idxmax()\n    best = df_results.iloc[best_idx]\n    \n    summary_data.append({\n        'Split Ratio': split_ratio,\n        'Best Model': best['Model'],\n        'Best Feature': best['Feature Type'],\n        'Best N-gram': best['N-gram'],\n        'Best Sampling': best['Sampling'],\n        'Best F1-Score': best['F1-Score'],\n        'Best Accuracy': best['Accuracy'],\n        'Best ROC AUC': best['ROC AUC']\n    })\n\nsummary_df = pd.DataFrame(summary_data)\nsummary_df.to_csv('results_absa_comparison/summary_all_splits.csv', index=False)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"EXPERIMENT COMPLETED SUCCESSFULLY!\")\nprint(\"=\"*80)\nprint(f\"\\nResults saved in: results_absa_comparison/\")\nprint(f\"- Individual reports for each split ratio\")\nprint(f\"- CSV files with detailed metrics\")\nprint(f\"- Visualizations (confusion matrices, ROC curves, metrics comparison)\")\nprint(f\"- Trained models saved in results_absa_comparison/models/\")\nprint(\"\\nSummary of Best Models Across Splits:\")\nprint(summary_df.to_string(index=False))","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8. Display Summary Results","metadata":{"editable":false}},{"cell_type":"code","source":"# Display summary DataFrame\nsummary_df","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null}]}